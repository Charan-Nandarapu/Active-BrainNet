{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"L1PgftsS-t0H","tags":[],"outputId":"cf0e7889-8bd1-4f37-9b24-c1cca7c63bb5"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-13 16:24:33.731720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-03-13 16:24:35.551483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["a#Import the neccessary packages.\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import os\n","import sys\n","import time\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import classification_report, log_loss, accuracy_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, GlobalAveragePooling2D, Dense, Add\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import load_model, save_model\n","from tqdm import tqdm\n","from sklearn.manifold import TSNE\n","from scipy.stats import multivariate_normal\n","from mpl_toolkits.mplot3d import Axes3D\n"]},{"cell_type":"markdown","metadata":{"id":"z_MxwDOO-t0L"},"source":["# Labeled\n"," Data Pool (train images, train labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1599,"status":"ok","timestamp":1704196529335,"user":{"displayName":"cn gp","userId":"05585428423279754186"},"user_tz":-330},"id":"UOIldgd9-t0M","outputId":"dec0e80f-9400-4bc2-cc60-61cb3e8d1192","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["len of the training data =  310\n","310\n","310\n","(310, 150, 150, 3)\n","(310, 4)\n"]},{"data":{"text/plain":["\"file_path = '7023_085/img_arr_7023_(0.85)_12.pkl'\\nwith open(file_path, 'rb') as file:\\n    images = pickle.load(file)\\n    \\nfile_path = '7023_085/img_lab_7023_(0.85)_12.pkl'\\nwith open(file_path, 'rb') as file:\\n    labels = pickle.load(file)\\n    \\nprint(len(images))\\nprint(len(labels))\""]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["file_path = 'Data_Random_sampled(arr, lab)_3p.pkl'     #Importing random sampled 3% labelled dataset.\n","import pickle\n","with open(file_path, 'rb') as file:                    #loding the data in pickle file\n","    train_data = pickle.load(file)                     #storing teh data in list variable\n","\n","print(\"len of the training data = \", len(train_data))\n","train_images,train_labels = zip(*train_data)           #extracting the numpy array image data and corresponding labels to two different list variables.\n","train_images = np.array(train_images)                  #converting to numpy array\n","train_labels = np.array(train_labels)                  #converting to numpy array\n","train_labels = to_categorical(train_labels)            #Converting labels to categorical values using one-hot encoding format\n","print(train_images.shape)                              #cross veriyfying the lenght of train images and train labels\n","print(train_labels.shape)                              #both lengths should be the same"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1704196529335,"user":{"displayName":"cn gp","userId":"05585428423279754186"},"user_tz":-330},"id":"vl_YG9Tx-t0M","outputId":"b60528ea-a738-43dc-bf0c-8f7be5af7815","tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-13 16:24:40.311854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.827061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.827281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.828182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.828369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.828536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.924689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.924896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.925065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2024-03-13 16:24:40.925178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20931 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," densenet201 (Functional)    (None, 1920)              18321984  \n","                                                                 \n"," dense (Dense)               (None, 128)               245888    \n","                                                                 \n"," dense_1 (Dense)             (None, 4)                 516       \n","                                                                 \n","=================================================================\n","Total params: 18568388 (70.83 MB)\n","Trainable params: 18339332 (69.96 MB)\n","Non-trainable params: 229056 (894.75 KB)\n","_________________________________________________________________\n"]}],"source":["model=load_model('DenseNet201_100_epochs.h5')  #Importing the pre-trained model which is trained using 3 % of lableled dataset\n","model.summary()                                #summary of the Model. Gives all the details of layers and input/output shapes"]},{"cell_type":"markdown","source":["#UNCERTAINITY SCORES..\n"],"metadata":{"id":"RTknWWDAjHFW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvOlaucD-t0O","tags":[]},"outputs":[],"source":["\n","def calculate_informativeness_scores(predictions):  #function to calculate informativeness and representativeness\n","    print(\"\\nCalculating informativeness scores...\")\n","    entropy = -np.sum(predictions * np.log(predictions + 1e-10), axis=1)     # Calculating entropy of predicted probabilities\n","    top_class_probabilities = np.max(predictions, axis=1)                    # picking the highes value in predicted probabilities\n","    second_top_class_probabilities = np.partition(predictions, -2)[:, -2]    # picking the second highes pvalue in predicted probabilities\n","    margin = top_class_probabilities - second_top_class_probabilities        # Calculating the margin\n","    weighted_sum = entropy + (1- margin)                                     #  sum of entropy and inverse of 1-margin (margin is inversely propotional to Entropy)\n","    print(\"\\nInformativeness scores calculated successfully\")\n","    return weighted_sum                                                      # returning the value of Informativeness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHrpZCqehOBk"},"outputs":[],"source":["file_path = 'Variances.pkl'                                                         #Loading the claclulated representativensess score for entire unlabaled data pool\n","with open(file_path, 'rb') as file:                                                 #opening file using python\n","    representativeness_scores = pickle.load(file)                                   #Storing the representaiveness scores in a list variable\n","\n","print(f'The list has been loaded')\n","len(representativeness_scores)                                                      #length should equal to number of unlabled samples in the unlabled data pool\n","representativeness_scores=np.array(representativeness_scores)                       #converting to numpy arrays\n","representativeness_scores=representativeness_scores/max(representativeness_scores)  #normalising the representativeness scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEsgFd7L-t0Q","tags":[]},"outputs":[],"source":["#Calculating Combined uncertatinity scores (Parent step)\n","def calculate_combined_Uncertainity_scores(predictions,representativeness_scores):  #function calculate combined uncertainity scores\n","    informativeness_scores = calculate_informativeness_scores(predictions)          #retriving the informativeness scores\n","    informativeness_scores=informativeness_scores/max(informativeness_scores)       #retriving the representativeness scores\n","    alpha=0.5                                                                       #weight for informativeness\n","    beta=0.5                                                                        #weight for representativeness\n","    combined_scores=alpha*informativeness_scores+beta*representativeness_scores     #weighted sum\n","    print(\"\\nCalculating combined scores...\")\n","    print(\"The combined scores are calculated...\")\n","    return combined_scores                                                          #returning the uncertaininty scores to the active learning funtion"]},{"cell_type":"markdown","source":["#Intialising the active learning"],"metadata":{"id":"bgcvnqQoBORk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xpJQcBy-t0N","tags":[]},"outputs":[],"source":["unlabeled_data_path='data(7023)_with_labels.pkl'     #Fetching the unlabeled data pool.\n","    print(\"Fetching Unlabeled data pool...\")\n","with open(unlabeled_data_path, 'rb') as file:        #loading the unlabled data pool\n","    unlabeled_data = pickle.load(file)               #Storing the data in list varibale\n","print(\"\\n\\ncompleted fetching unlabeled data!!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLnI_0oM-t0Q","scrolled":true},"outputs":[],"source":["active_cycle_number = 1                                                         #initialising active learning cycle number\n","while active_cycle_number<=25:                                                  #While loop for running through cycles\n","    print(\"\\nIntialising ACTIVE LEARNING cycle no. \",active_cycle_number)       #prompt\n","    predictions = model.predict(X_arr)                                          #Feeding unlabled data pool to model and acquiring prediction probabilities\n","    combined_uncertainity_scores = calculate_combined_Uncertainity_scores(predictions,representativeness_scores) #retriving uncertainity scores\n","    print(\"Noralising the scores...\")\n","    normalized_scores = combined_uncertainity_scores / max(combined_uncertainity_scores) #normalising thge uncertainity scores\n","    print(\"Selecting the most informative samples..\")                           #prompt\n","    threshold = 0.95                                                            #setting iniatial threshold value\n","    while True:                                                                 #initialising \"DYNAMIC THRESHOLDING\"\n","        selected_indices = np.where(normalized_scores > threshold)[0]           #select all the normalised uncertaininty scores indices where the value is leass than current threshold\n","        if len(selected_indices) < 50:                                          #checking if required minimum number of samples are selected or not\n","            threshold -= 0.05                                                   #if not, we should decrease the value of threshold by 0.05\n","        else:                                                                   #else block ( which means, enough number of samples are selected.)\n","            break                                                               #break the dynamic threshold loop\n","    selected_data = [(X_arr[i], img_labels[i]) for i in selected_indices]       #using the selected indices above, we are fetching the data from unlabled pool data of image arrays\n","    print(\"\\nSelection of informative samples is complete!..\")                  #prompt\n","    #GUI WILL COME HERE\n","    print(\"\\nNumber of Informativeness samples selected in the current loop = \",len(selected_data)) #PROMPT\n","    selected_data_tuples = [(tuple(X), img_label) for X, img_label in selected_data] #converting the extracted dat into tuples of labels and images\n","    print(\"The number of selected samples for retraining the model = \", len(selected_data)) #prompt\n","    print(\"\\nPress Enter to continue training model...!!\")                      #prompt\n","    selected_images, selected_labels =zip(*selected_data)                       #separating images and labels into two different lists\n","    print(\"\\nAppending the sampled data pool to the existing labeled data..\")   #prompt\n","\n","    # Create a mask to filter out selected indices\n","    '''mask = np.ones(len(X_arr), dtype=bool)                                   #to be verified\n","    mask[selected_indices] = False                                              #to be verified\n","    X_arr = X_arr[mask]                                                         #to be verified\n","    representativeness_scores = representativeness_scores[mask]                 #to be verified\n","    print(\"Reduced - Number of samples in unlabled data pool: \",len(X_arr))     #to be verified'''\n","\n","    images = images + selected_images                                           #appending new selected images to exisiting labeled data pool\n","    labels = labels + selected_labels                                           # appending corresponding labels to exisiting labeled data pool\n","\n","\n","    train_images= np.array(images)                                              #converting to numpy arrays\n","    train_images= train_images.reshape(-1,150,150,3)                            #reshaping the new set to fit into models input shape\n","    train_labels = np.array(labels)                                             #converting to numpy arrays\n","    train_labels = to_categorical(labels, num_classes=4)                        #converting labels as categorical values using onehot encoding method.\n","\n","\n","    print(\"Now, The total number of data samples for retraining the model is = \",len(train_images)) #prompt\n","    print(\"\\nThe total percentage of data used for current cycle = \", (len(train_images)/10357))    #prompt\n","\n","    #Training the active learning model\n","    print(\"\\nPreparing for training a model..\")\n","    #spliting data for training and testing using Train_test_split function in python\n","    X_train, X_test, y_train, y_test = train_test_split(train_images,train_labels, test_size=0.2,random_state=44)\n","    #data agumentation\n","    data_aug = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=20, zoom_range=0.2,width_shift_range=0.2, height_shift_range=0.2, shear_range=0.1, fill_mode=\"nearest\")\n","    print(\"\\nTraining the Model...\")\n","    #Training the model existing model with new data with 50 epochs and 32 batch size\n","    history = model.fit(data_aug.flow(X_train, y_train, batch_size=32), validation_data=(X_test, y_test), epochs=50)\n","    y_pred=model.predict(X_test)                                                #testing the model\n","    print(\"\\nTraining Completed!!..\")                                           #prompt\n","    print(\"\\n\\nTest Results .....\")                                             #prompt\n","    pred = np.argmax(y_pred,axis=1)                                             #prediction classes extraction using argmax function to calculate accuracy\n","    original = np.argmax(y_test,axis=1)                                         #taking the original labels of test dataset to test\n","    print(classification_report(original,pred))                                 #printing classification report using predicted class labels and original class labels\n","\n","    #training accuracy and validation accuracy graph\n","    print('\\n\\nTraining accuracy and validation accuracy graph ..')\n","    epochs = range(1, len(history.history['accuracy']) + 1)\n","    plt.plot(epochs, history.history['accuracy'], 'r', label='Accuracy of Training data')\n","    plt.plot(epochs, history.history['val_accuracy'], 'b', label='Accuracy of Validation data')\n","    plt.title('Training vs validation accuracy')\n","    plt.legend(loc=0)\n","    plt.figure()\n","    plt.show()\n","\n","\n","    #training loss and validation loss graph\n","    print('\\n\\nTraining loss and validation loss graph....')\n","    epochs = range(1, len(history.history['accuracy']) + 1)\n","    plt.plot(epochs, history.history['loss'], 'r', label='Loss of Training data')\n","    plt.plot(epochs, history.history['val_loss'], 'b', label='Loss of Validation data')\n","    plt.title('Training vs validation loss')\n","    plt.legend(loc=0)\n","    plt.figure()\n","    plt.show()\n","\n","\n","    #Accuracy printing\n","    from sklearn.metrics import accuracy_score\n","    accuracy = accuracy_score(original, pred)\n","    print(\"\\nAccuracy: \", accuracy)\n","\n","\n","    #Saving The model\n","    model.save(os.path.join(\"D_095\", f'model_(D)_{active_cycle_number}.h5'))    #save the model for each iteration.\n","    active_cycle_number += 1                                                    #proceeding to next active learning cycle."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2AbduwGhOBm"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"conda_tensorflow2_p310","language":"python","name":"conda_tensorflow2_p310"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}